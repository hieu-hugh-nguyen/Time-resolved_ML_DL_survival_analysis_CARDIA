{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_features = 'all'\n",
    "term_pred = 'long_term'\n",
    "data_file_name = 'data_after_y10_origin_at_10'\n",
    "training_id_file_name = 'all_training_ID_outerloop_cohort_10_26'\n",
    "endpt = 16;\n",
    "eval_times = 365.25*np.r_[np.arange(2, endpt+1, 2)]\n",
    "l2_final=0.1\n",
    "# work_dir = '/home/idies/workspace/Storage/hnguye78/persistent/CARDIA_project/cvd_outcome_rerun2'\n",
    "work_dir = 'U:/Hieu/CARDIA_project/CARDIA_project/cvd_outcome_rerun_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting theano\n",
      "  Downloading Theano-1.0.5.tar.gz (2.8 MB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from theano) (1.19.2)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from theano) (1.5.2)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from theano) (1.15.0)\n",
      "Building wheels for collected packages: theano\n",
      "  Building wheel for theano (setup.py): started\n",
      "  Building wheel for theano (setup.py): finished with status 'done'\n",
      "  Created wheel for theano: filename=Theano-1.0.5-py3-none-any.whl size=2668111 sha256=46978b0474e895fb754ef957d7456477cf47fa0c733193c9491fa1bfc9ae35cc\n",
      "Note: you may need to restart the kernel to use updated packages.  Stored in directory: c:\\users\\hnguye78\\appdata\\local\\pip\\cache\\wheels\\84\\cb\\19\\235b5b10d89b4621f685112f8762681570a9fa14dc1ce904d9\n",
      "Successfully built theano\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for backports-tempfile: [Errno 13] Permission denied: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\backports.tempfile-1.0.dist-info\\\\METADATA'\n",
      "WARNING: Error parsing requirements for backports-functools-lru-cache: [Errno 13] Permission denied: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\backports.functools_lru_cache-1.6.1.dist-info\\\\METADATA'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Installing collected packages: theano\n",
      "Successfully installed theano-1.0.5\n"
     ]
    }
   ],
   "source": [
    "# %pip install tensorflow\n",
    "# %pip install lifelines\n",
    "\n",
    "# !y | pip uninstall statsmodels \n",
    "# %pip install statsmodels==0.11.0\n",
    "\n",
    "# %pip install keras\n",
    "# %pip install theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import math\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import concordance_index\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'U:/Hieu/CARDIA_project/CARDIA_project/cvd_outcome_rerun_2/code/snippet'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snippets_dir\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "# snippets_dir = '/home/idies/workspace/Storage/hnguye78/persistent/CARDIA_project/cvd_outcome_rerun'+ '/code/snippet'\n",
    "snippets_dir = work_dir+'/cardia_rerun_2_code/snippet'\n",
    "import os, sys\n",
    "sys.path.append(os.path.abspath(snippets_dir))\n",
    "import cox_nnet\n",
    "from cox import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dir = work_dir+ '/csv_files'\n",
    "\n",
    "# load data:\n",
    "data_full = pd.read_csv(load_dir+'/'+data_file_name+'.csv')\n",
    "data_full = data_full.select_dtypes(include =[np.number])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>event</th>\n",
       "      <th>time</th>\n",
       "      <th>sex</th>\n",
       "      <th>race</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>hdl</th>\n",
       "      <th>hbp.medication</th>\n",
       "      <th>smoker</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>...</th>\n",
       "      <th>C08MELAG_YEAR</th>\n",
       "      <th>C08SKNAG_YEAR</th>\n",
       "      <th>C08BRNAG_YEAR</th>\n",
       "      <th>C08STMAG_YEAR</th>\n",
       "      <th>C08OCAAG_YEAR</th>\n",
       "      <th>C08GALDA_YEAR</th>\n",
       "      <th>C08PRGAG_YEAR</th>\n",
       "      <th>C08KYSAG_YEAR</th>\n",
       "      <th>C08URNAG_YEAR</th>\n",
       "      <th>count_na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100016012504</td>\n",
       "      <td>0</td>\n",
       "      <td>5258.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.048673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100023004268</td>\n",
       "      <td>0</td>\n",
       "      <td>5560.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.055310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100033323702</td>\n",
       "      <td>0</td>\n",
       "      <td>5381.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.077434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100056526386</td>\n",
       "      <td>0</td>\n",
       "      <td>5511.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>199.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100061300991</td>\n",
       "      <td>0</td>\n",
       "      <td>5403.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>200.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4195</th>\n",
       "      <td>416761219907</td>\n",
       "      <td>0</td>\n",
       "      <td>5439.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.048673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4196</th>\n",
       "      <td>416771521620</td>\n",
       "      <td>0</td>\n",
       "      <td>5586.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>204.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4197</th>\n",
       "      <td>416783315386</td>\n",
       "      <td>0</td>\n",
       "      <td>5563.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.112832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4198</th>\n",
       "      <td>416796224310</td>\n",
       "      <td>0</td>\n",
       "      <td>5452.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>164.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.070796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4199</th>\n",
       "      <td>416817227898</td>\n",
       "      <td>0</td>\n",
       "      <td>5512.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>204.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.115044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4200 rows × 457 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID  event    time  sex  race  cholesterol   hdl  \\\n",
       "0     100016012504      0  5258.5    0     0        186.0  38.0   \n",
       "1     100023004268      0  5560.5    0     0        166.0  43.0   \n",
       "2     100033323702      0  5381.5    0     0        234.0  52.0   \n",
       "3     100056526386      0  5511.5    0     1        199.0  44.0   \n",
       "4     100061300991      0  5403.5    0     1        200.0  40.0   \n",
       "...            ...    ...     ...  ...   ...          ...   ...   \n",
       "4195  416761219907      0  5439.5    1     0        225.0  57.0   \n",
       "4196  416771521620      0  5586.5    0     1        204.0  76.0   \n",
       "4197  416783315386      0  5563.5    1     0        159.0  58.0   \n",
       "4198  416796224310      0  5452.5    1     1        164.0  55.0   \n",
       "4199  416817227898      0  5512.5    0     1        204.0  43.0   \n",
       "\n",
       "      hbp.medication  smoker  diabetes  ...  C08MELAG_YEAR  C08SKNAG_YEAR  \\\n",
       "0                  0       1         0  ...              0              0   \n",
       "1                  0       0         0  ...              0              0   \n",
       "2                  0       0         0  ...              0              0   \n",
       "3                  0       0         0  ...              0              0   \n",
       "4                  0       1         0  ...              0              0   \n",
       "...              ...     ...       ...  ...            ...            ...   \n",
       "4195               0       0         0  ...              0              0   \n",
       "4196               0       0         0  ...              0              0   \n",
       "4197               0       0         0  ...              0              0   \n",
       "4198               0       0         0  ...              0              0   \n",
       "4199               0       1         0  ...              0              0   \n",
       "\n",
       "      C08BRNAG_YEAR  C08STMAG_YEAR  C08OCAAG_YEAR  C08GALDA_YEAR  \\\n",
       "0                 0              0              0              0   \n",
       "1                 0              0              0              0   \n",
       "2                 0              0              0              0   \n",
       "3                 0              0              0              0   \n",
       "4                 0              0              0              0   \n",
       "...             ...            ...            ...            ...   \n",
       "4195              0              0              0              0   \n",
       "4196              0              0              0              0   \n",
       "4197              0              0              0              0   \n",
       "4198              0              0              0              0   \n",
       "4199              0              0              0              0   \n",
       "\n",
       "      C08PRGAG_YEAR  C08KYSAG_YEAR  C08URNAG_YEAR  count_na  \n",
       "0                 0              0              0  0.048673  \n",
       "1                 0              0              0  0.055310  \n",
       "2                 0              0              0  0.077434  \n",
       "3                 0              0              0  0.008850  \n",
       "4                 0              0              0  0.000000  \n",
       "...             ...            ...            ...       ...  \n",
       "4195             14              0              0  0.048673  \n",
       "4196              0              0              0  0.004425  \n",
       "4197              8              0              0  0.112832  \n",
       "4198             14              0              0  0.070796  \n",
       "4199              0              0              0  0.115044  \n",
       "\n",
       "[4200 rows x 457 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training id:\n",
    "loaddir = work_dir+ '/csv_files'\n",
    "trainingid_all = pd.read_csv(loaddir+'/'+training_id_file_name+'.csv')\n",
    "\n",
    "## standardize feature space, then merge back to label space:\n",
    "feature_space = data_full.drop(['ID','event','time'], axis = 1)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(feature_space)\n",
    "scaled_feature_space = scaler.transform(feature_space)\n",
    "scaled_feature_space_df = pd.DataFrame(data=scaled_feature_space[0:,0:])\n",
    "scaled_feature_space_df.insert(0, 'ID', data_full['ID'], True)\n",
    "label = data_full.loc[:,['ID','event','time']]\n",
    "data_full = pd.merge(label, scaled_feature_space_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>event</th>\n",
       "      <th>time</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>444</th>\n",
       "      <th>445</th>\n",
       "      <th>446</th>\n",
       "      <th>447</th>\n",
       "      <th>448</th>\n",
       "      <th>449</th>\n",
       "      <th>450</th>\n",
       "      <th>451</th>\n",
       "      <th>452</th>\n",
       "      <th>453</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100016012504</td>\n",
       "      <td>0</td>\n",
       "      <td>5258.5</td>\n",
       "      <td>-1.120553</td>\n",
       "      <td>-0.959853</td>\n",
       "      <td>0.239613</td>\n",
       "      <td>-1.105713</td>\n",
       "      <td>-0.123404</td>\n",
       "      <td>1.596965</td>\n",
       "      <td>-0.14629</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022543</td>\n",
       "      <td>-0.0246</td>\n",
       "      <td>-0.021827</td>\n",
       "      <td>-0.015432</td>\n",
       "      <td>-0.055668</td>\n",
       "      <td>-0.086914</td>\n",
       "      <td>-0.630124</td>\n",
       "      <td>-0.089138</td>\n",
       "      <td>-0.121811</td>\n",
       "      <td>0.774994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100023004268</td>\n",
       "      <td>0</td>\n",
       "      <td>5560.5</td>\n",
       "      <td>-1.120553</td>\n",
       "      <td>-0.959853</td>\n",
       "      <td>-0.356702</td>\n",
       "      <td>-0.746751</td>\n",
       "      <td>-0.123404</td>\n",
       "      <td>-0.626188</td>\n",
       "      <td>-0.14629</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022543</td>\n",
       "      <td>-0.0246</td>\n",
       "      <td>-0.021827</td>\n",
       "      <td>-0.015432</td>\n",
       "      <td>-0.055668</td>\n",
       "      <td>-0.086914</td>\n",
       "      <td>-0.630124</td>\n",
       "      <td>-0.089138</td>\n",
       "      <td>-0.121811</td>\n",
       "      <td>0.981136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100033323702</td>\n",
       "      <td>0</td>\n",
       "      <td>5381.5</td>\n",
       "      <td>-1.120553</td>\n",
       "      <td>-0.959853</td>\n",
       "      <td>1.670768</td>\n",
       "      <td>-0.100620</td>\n",
       "      <td>-0.123404</td>\n",
       "      <td>-0.626188</td>\n",
       "      <td>-0.14629</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022543</td>\n",
       "      <td>-0.0246</td>\n",
       "      <td>-0.021827</td>\n",
       "      <td>-0.015432</td>\n",
       "      <td>-0.055668</td>\n",
       "      <td>-0.086914</td>\n",
       "      <td>-0.630124</td>\n",
       "      <td>-0.089138</td>\n",
       "      <td>-0.121811</td>\n",
       "      <td>1.668274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100056526386</td>\n",
       "      <td>0</td>\n",
       "      <td>5511.5</td>\n",
       "      <td>-1.120553</td>\n",
       "      <td>1.041826</td>\n",
       "      <td>0.627217</td>\n",
       "      <td>-0.674959</td>\n",
       "      <td>-0.123404</td>\n",
       "      <td>-0.626188</td>\n",
       "      <td>-0.14629</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022543</td>\n",
       "      <td>-0.0246</td>\n",
       "      <td>-0.021827</td>\n",
       "      <td>-0.015432</td>\n",
       "      <td>-0.055668</td>\n",
       "      <td>-0.086914</td>\n",
       "      <td>-0.630124</td>\n",
       "      <td>-0.089138</td>\n",
       "      <td>-0.121811</td>\n",
       "      <td>-0.461855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100061300991</td>\n",
       "      <td>0</td>\n",
       "      <td>5403.5</td>\n",
       "      <td>-1.120553</td>\n",
       "      <td>1.041826</td>\n",
       "      <td>0.657033</td>\n",
       "      <td>-0.962128</td>\n",
       "      <td>-0.123404</td>\n",
       "      <td>1.596965</td>\n",
       "      <td>-0.14629</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022543</td>\n",
       "      <td>-0.0246</td>\n",
       "      <td>-0.021827</td>\n",
       "      <td>-0.015432</td>\n",
       "      <td>-0.055668</td>\n",
       "      <td>-0.086914</td>\n",
       "      <td>-0.630124</td>\n",
       "      <td>-0.089138</td>\n",
       "      <td>-0.121811</td>\n",
       "      <td>-0.736711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4195</th>\n",
       "      <td>416761219907</td>\n",
       "      <td>0</td>\n",
       "      <td>5439.5</td>\n",
       "      <td>0.892416</td>\n",
       "      <td>-0.959853</td>\n",
       "      <td>1.402426</td>\n",
       "      <td>0.258341</td>\n",
       "      <td>-0.123404</td>\n",
       "      <td>-0.626188</td>\n",
       "      <td>-0.14629</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022543</td>\n",
       "      <td>-0.0246</td>\n",
       "      <td>-0.021827</td>\n",
       "      <td>-0.015432</td>\n",
       "      <td>-0.055668</td>\n",
       "      <td>-0.086914</td>\n",
       "      <td>1.966503</td>\n",
       "      <td>-0.089138</td>\n",
       "      <td>-0.121811</td>\n",
       "      <td>0.774994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4196</th>\n",
       "      <td>416771521620</td>\n",
       "      <td>0</td>\n",
       "      <td>5586.5</td>\n",
       "      <td>-1.120553</td>\n",
       "      <td>1.041826</td>\n",
       "      <td>0.776296</td>\n",
       "      <td>1.622396</td>\n",
       "      <td>-0.123404</td>\n",
       "      <td>-0.626188</td>\n",
       "      <td>-0.14629</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022543</td>\n",
       "      <td>-0.0246</td>\n",
       "      <td>-0.021827</td>\n",
       "      <td>-0.015432</td>\n",
       "      <td>-0.055668</td>\n",
       "      <td>-0.086914</td>\n",
       "      <td>-0.630124</td>\n",
       "      <td>-0.089138</td>\n",
       "      <td>-0.121811</td>\n",
       "      <td>-0.599283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4197</th>\n",
       "      <td>416783315386</td>\n",
       "      <td>0</td>\n",
       "      <td>5563.5</td>\n",
       "      <td>0.892416</td>\n",
       "      <td>-0.959853</td>\n",
       "      <td>-0.565412</td>\n",
       "      <td>0.330134</td>\n",
       "      <td>-0.123404</td>\n",
       "      <td>-0.626188</td>\n",
       "      <td>-0.14629</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022543</td>\n",
       "      <td>-0.0246</td>\n",
       "      <td>-0.021827</td>\n",
       "      <td>-0.015432</td>\n",
       "      <td>-0.055668</td>\n",
       "      <td>-0.086914</td>\n",
       "      <td>0.853663</td>\n",
       "      <td>-0.089138</td>\n",
       "      <td>-0.121811</td>\n",
       "      <td>2.767696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4198</th>\n",
       "      <td>416796224310</td>\n",
       "      <td>0</td>\n",
       "      <td>5452.5</td>\n",
       "      <td>0.892416</td>\n",
       "      <td>1.041826</td>\n",
       "      <td>-0.416334</td>\n",
       "      <td>0.114757</td>\n",
       "      <td>-0.123404</td>\n",
       "      <td>-0.626188</td>\n",
       "      <td>-0.14629</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022543</td>\n",
       "      <td>-0.0246</td>\n",
       "      <td>-0.021827</td>\n",
       "      <td>-0.015432</td>\n",
       "      <td>-0.055668</td>\n",
       "      <td>-0.086914</td>\n",
       "      <td>1.966503</td>\n",
       "      <td>-0.089138</td>\n",
       "      <td>-0.121811</td>\n",
       "      <td>1.462133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4199</th>\n",
       "      <td>416817227898</td>\n",
       "      <td>0</td>\n",
       "      <td>5512.5</td>\n",
       "      <td>-1.120553</td>\n",
       "      <td>1.041826</td>\n",
       "      <td>0.776296</td>\n",
       "      <td>-0.746751</td>\n",
       "      <td>-0.123404</td>\n",
       "      <td>1.596965</td>\n",
       "      <td>-0.14629</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022543</td>\n",
       "      <td>-0.0246</td>\n",
       "      <td>-0.021827</td>\n",
       "      <td>-0.015432</td>\n",
       "      <td>-0.055668</td>\n",
       "      <td>-0.086914</td>\n",
       "      <td>-0.630124</td>\n",
       "      <td>-0.089138</td>\n",
       "      <td>-0.121811</td>\n",
       "      <td>2.836410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4200 rows × 457 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID  event    time         0         1         2         3  \\\n",
       "0     100016012504      0  5258.5 -1.120553 -0.959853  0.239613 -1.105713   \n",
       "1     100023004268      0  5560.5 -1.120553 -0.959853 -0.356702 -0.746751   \n",
       "2     100033323702      0  5381.5 -1.120553 -0.959853  1.670768 -0.100620   \n",
       "3     100056526386      0  5511.5 -1.120553  1.041826  0.627217 -0.674959   \n",
       "4     100061300991      0  5403.5 -1.120553  1.041826  0.657033 -0.962128   \n",
       "...            ...    ...     ...       ...       ...       ...       ...   \n",
       "4195  416761219907      0  5439.5  0.892416 -0.959853  1.402426  0.258341   \n",
       "4196  416771521620      0  5586.5 -1.120553  1.041826  0.776296  1.622396   \n",
       "4197  416783315386      0  5563.5  0.892416 -0.959853 -0.565412  0.330134   \n",
       "4198  416796224310      0  5452.5  0.892416  1.041826 -0.416334  0.114757   \n",
       "4199  416817227898      0  5512.5 -1.120553  1.041826  0.776296 -0.746751   \n",
       "\n",
       "             4         5        6  ...       444     445       446       447  \\\n",
       "0    -0.123404  1.596965 -0.14629  ... -0.022543 -0.0246 -0.021827 -0.015432   \n",
       "1    -0.123404 -0.626188 -0.14629  ... -0.022543 -0.0246 -0.021827 -0.015432   \n",
       "2    -0.123404 -0.626188 -0.14629  ... -0.022543 -0.0246 -0.021827 -0.015432   \n",
       "3    -0.123404 -0.626188 -0.14629  ... -0.022543 -0.0246 -0.021827 -0.015432   \n",
       "4    -0.123404  1.596965 -0.14629  ... -0.022543 -0.0246 -0.021827 -0.015432   \n",
       "...        ...       ...      ...  ...       ...     ...       ...       ...   \n",
       "4195 -0.123404 -0.626188 -0.14629  ... -0.022543 -0.0246 -0.021827 -0.015432   \n",
       "4196 -0.123404 -0.626188 -0.14629  ... -0.022543 -0.0246 -0.021827 -0.015432   \n",
       "4197 -0.123404 -0.626188 -0.14629  ... -0.022543 -0.0246 -0.021827 -0.015432   \n",
       "4198 -0.123404 -0.626188 -0.14629  ... -0.022543 -0.0246 -0.021827 -0.015432   \n",
       "4199 -0.123404  1.596965 -0.14629  ... -0.022543 -0.0246 -0.021827 -0.015432   \n",
       "\n",
       "           448       449       450       451       452       453  \n",
       "0    -0.055668 -0.086914 -0.630124 -0.089138 -0.121811  0.774994  \n",
       "1    -0.055668 -0.086914 -0.630124 -0.089138 -0.121811  0.981136  \n",
       "2    -0.055668 -0.086914 -0.630124 -0.089138 -0.121811  1.668274  \n",
       "3    -0.055668 -0.086914 -0.630124 -0.089138 -0.121811 -0.461855  \n",
       "4    -0.055668 -0.086914 -0.630124 -0.089138 -0.121811 -0.736711  \n",
       "...        ...       ...       ...       ...       ...       ...  \n",
       "4195 -0.055668 -0.086914  1.966503 -0.089138 -0.121811  0.774994  \n",
       "4196 -0.055668 -0.086914 -0.630124 -0.089138 -0.121811 -0.599283  \n",
       "4197 -0.055668 -0.086914  0.853663 -0.089138 -0.121811  2.767696  \n",
       "4198 -0.055668 -0.086914  1.966503 -0.089138 -0.121811  1.462133  \n",
       "4199 -0.055668 -0.086914 -0.630124 -0.089138 -0.121811  2.836410  \n",
       "\n",
       "[4200 rows x 457 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD 6:\n",
      "\n",
      "Using nesterov accelerated gradient\n",
      "training model\n",
      "cost: 4.498827, iteration: 0\n",
      "cost: 4.471642, iteration: 23\n",
      "cost: 4.434653, iteration: 46\n",
      "cost: 4.397111, iteration: 69\n",
      "cost: 4.359835, iteration: 92\n",
      "cost: 4.322895, iteration: 115\n",
      "cost: 4.286292, iteration: 138\n",
      "cost: 4.250025, iteration: 161\n",
      "cost: 4.214091, iteration: 184\n",
      "cost: 4.178487, iteration: 207\n",
      "cost: 4.143210, iteration: 230\n",
      "cost: 4.108256, iteration: 253\n",
      "cost: 4.073623, iteration: 276\n",
      "cost: 4.039308, iteration: 299\n",
      "cost: 4.005307, iteration: 322\n",
      "cost: 3.971619, iteration: 345\n",
      "cost: 3.938240, iteration: 368\n",
      "cost: 3.905167, iteration: 391\n",
      "cost: 3.872397, iteration: 414\n",
      "cost: 3.839929, iteration: 437\n",
      "cost: 3.807758, iteration: 460\n",
      "cost: 3.775882, iteration: 483\n",
      "cost: 3.744299, iteration: 506\n",
      "cost: 3.713006, iteration: 529\n",
      "cost: 3.682000, iteration: 552\n",
      "cost: 3.651278, iteration: 575\n",
      "cost: 3.620838, iteration: 598\n",
      "cost: 3.590678, iteration: 621\n",
      "cost: 3.560795, iteration: 644\n",
      "cost: 3.531185, iteration: 667\n",
      "cost: 3.501848, iteration: 690\n",
      "cost: 3.472779, iteration: 713\n",
      "cost: 3.443978, iteration: 736\n",
      "cost: 3.415440, iteration: 759\n",
      "cost: 3.387165, iteration: 782\n",
      "cost: 3.359149, iteration: 805\n",
      "cost: 3.331390, iteration: 828\n",
      "cost: 3.303885, iteration: 851\n",
      "cost: 3.276634, iteration: 874\n",
      "cost: 3.249632, iteration: 897\n",
      "cost: 3.222878, iteration: 920\n",
      "cost: 3.196370, iteration: 943\n",
      "cost: 3.170104, iteration: 966\n",
      "cost: 3.144080, iteration: 989\n",
      "cost: 3.118295, iteration: 1012\n",
      "cost: 3.092746, iteration: 1035\n",
      "cost: 3.067432, iteration: 1058\n",
      "cost: 3.042350, iteration: 1081\n",
      "cost: 3.017498, iteration: 1104\n",
      "cost: 2.992875, iteration: 1127\n",
      "cost: 2.968477, iteration: 1150\n",
      "cost: 2.944303, iteration: 1173\n",
      "cost: 2.920351, iteration: 1196\n",
      "cost: 2.896619, iteration: 1219\n",
      "cost: 2.873104, iteration: 1242\n",
      "cost: 2.849806, iteration: 1265\n",
      "cost: 2.826721, iteration: 1288\n",
      "cost: 2.803848, iteration: 1311\n",
      "cost: 2.781185, iteration: 1334\n",
      "cost: 2.758730, iteration: 1357\n",
      "cost: 2.736481, iteration: 1380\n",
      "cost: 2.714436, iteration: 1403\n",
      "cost: 2.692593, iteration: 1426\n",
      "cost: 2.670951, iteration: 1449\n",
      "cost: 2.649508, iteration: 1472\n",
      "cost: 2.628261, iteration: 1495\n",
      "cost: 2.607209, iteration: 1518\n",
      "cost: 2.586351, iteration: 1541\n",
      "cost: 2.565684, iteration: 1564\n",
      "cost: 2.545206, iteration: 1587\n",
      "cost: 2.524917, iteration: 1610\n",
      "cost: 2.504813, iteration: 1633\n",
      "cost: 2.484895, iteration: 1656\n",
      "cost: 2.465159, iteration: 1679\n",
      "cost: 2.445604, iteration: 1702\n",
      "cost: 2.426228, iteration: 1725\n",
      "cost: 2.407031, iteration: 1748\n",
      "cost: 2.388009, iteration: 1771\n",
      "cost: 2.369162, iteration: 1794\n",
      "cost: 2.350488, iteration: 1817\n",
      "cost: 2.331986, iteration: 1840\n",
      "cost: 2.313653, iteration: 1863\n",
      "cost: 2.295489, iteration: 1886\n",
      "cost: 2.277491, iteration: 1909\n",
      "cost: 2.259658, iteration: 1932\n",
      "cost: 2.241989, iteration: 1955\n",
      "cost: 2.224483, iteration: 1978\n",
      "cost: 2.207137, iteration: 2001\n",
      "cost: 2.189950, iteration: 2024\n",
      "cost: 2.172920, iteration: 2047\n",
      "cost: 2.156047, iteration: 2070\n",
      "cost: 2.139329, iteration: 2093\n",
      "cost: 2.122765, iteration: 2116\n",
      "cost: 2.106352, iteration: 2139\n",
      "cost: 2.090090, iteration: 2162\n",
      "cost: 2.073978, iteration: 2185\n",
      "cost: 2.058013, iteration: 2208\n",
      "cost: 2.042194, iteration: 2231\n",
      "cost: 2.026521, iteration: 2254\n",
      "cost: 2.010992, iteration: 2277\n",
      "cost: 1.995605, iteration: 2300\n",
      "cost: 1.980359, iteration: 2323\n",
      "cost: 1.965254, iteration: 2346\n",
      "cost: 1.950287, iteration: 2369\n",
      "cost: 1.935457, iteration: 2392\n",
      "cost: 1.920764, iteration: 2415\n",
      "cost: 1.906205, iteration: 2438\n",
      "cost: 1.891780, iteration: 2461\n",
      "cost: 1.877487, iteration: 2484\n",
      "cost: 1.863326, iteration: 2507\n",
      "cost: 1.849294, iteration: 2530\n",
      "cost: 1.835391, iteration: 2553\n",
      "cost: 1.821616, iteration: 2576\n",
      "cost: 1.807967, iteration: 2599\n",
      "cost: 1.794444, iteration: 2622\n",
      "cost: 1.781045, iteration: 2645\n",
      "cost: 1.767768, iteration: 2668\n",
      "cost: 1.754614, iteration: 2691\n",
      "cost: 1.741580, iteration: 2714\n",
      "cost: 1.728666, iteration: 2737\n",
      "cost: 1.715870, iteration: 2760\n",
      "cost: 1.703192, iteration: 2783\n",
      "cost: 1.690630, iteration: 2806\n",
      "cost: 1.678183, iteration: 2829\n",
      "cost: 1.665851, iteration: 2852\n",
      "cost: 1.653632, iteration: 2875\n",
      "cost: 1.641525, iteration: 2898\n",
      "cost: 1.629529, iteration: 2921\n",
      "cost: 1.617643, iteration: 2944\n",
      "cost: 1.605867, iteration: 2967\n",
      "cost: 1.594198, iteration: 2990\n",
      "cost: 1.582637, iteration: 3013\n",
      "cost: 1.571181, iteration: 3036\n",
      "cost: 1.559831, iteration: 3059\n",
      "cost: 1.548585, iteration: 3082\n",
      "cost: 1.537442, iteration: 3105\n",
      "cost: 1.526402, iteration: 3128\n",
      "cost: 1.515462, iteration: 3151\n",
      "cost: 1.504623, iteration: 3174\n",
      "cost: 1.493884, iteration: 3197\n",
      "cost: 1.483243, iteration: 3220\n",
      "cost: 1.472700, iteration: 3243\n",
      "cost: 1.462254, iteration: 3266\n",
      "cost: 1.451903, iteration: 3289\n",
      "cost: 1.441648, iteration: 3312\n",
      "cost: 1.431487, iteration: 3335\n",
      "cost: 1.421418, iteration: 3358\n",
      "cost: 1.411443, iteration: 3381\n",
      "cost: 1.401559, iteration: 3404\n",
      "cost: 1.391765, iteration: 3427\n",
      "cost: 1.382062, iteration: 3450\n",
      "cost: 1.372447, iteration: 3473\n",
      "cost: 1.362921, iteration: 3496\n",
      "cost: 1.353482, iteration: 3519\n",
      "cost: 1.344130, iteration: 3542\n",
      "cost: 1.334864, iteration: 3565\n",
      "cost: 1.325683, iteration: 3588\n",
      "cost: 1.316586, iteration: 3611\n",
      "cost: 1.307572, iteration: 3634\n",
      "cost: 1.298642, iteration: 3657\n",
      "cost: 1.289793, iteration: 3680\n",
      "cost: 1.281025, iteration: 3703\n",
      "cost: 1.272338, iteration: 3726\n",
      "cost: 1.263731, iteration: 3749\n",
      "cost: 1.255203, iteration: 3772\n",
      "cost: 1.246752, iteration: 3795\n",
      "cost: 1.238380, iteration: 3818\n",
      "cost: 1.230084, iteration: 3841\n",
      "cost: 1.221865, iteration: 3864\n",
      "cost: 1.213721, iteration: 3887\n",
      "cost: 1.205651, iteration: 3910\n",
      "cost: 1.197656, iteration: 3933\n",
      "cost: 1.189734, iteration: 3956\n",
      "cost: 1.181885, iteration: 3979\n",
      "cost: 1.174108, iteration: 4002\n",
      "cost: 1.166402, iteration: 4025\n",
      "cost: 1.158767, iteration: 4048\n",
      "cost: 1.151202, iteration: 4071\n",
      "cost: 1.143707, iteration: 4094\n",
      "cost: 1.136280, iteration: 4117\n",
      "cost: 1.128922, iteration: 4140\n",
      "cost: 1.121631, iteration: 4163\n",
      "cost: 1.114407, iteration: 4186\n",
      "cost: 1.107249, iteration: 4209\n",
      "cost: 1.100157, iteration: 4232\n",
      "cost: 1.093130, iteration: 4255\n",
      "cost: 1.086168, iteration: 4278\n",
      "cost: 1.079269, iteration: 4301\n",
      "cost: 1.072434, iteration: 4324\n",
      "cost: 1.065661, iteration: 4347\n",
      "cost: 1.058951, iteration: 4370\n",
      "cost: 1.052302, iteration: 4393\n",
      "cost: 1.045715, iteration: 4416\n",
      "cost: 1.039187, iteration: 4439\n",
      "cost: 1.032720, iteration: 4462\n",
      "cost: 1.026312, iteration: 4485\n",
      "cost: 1.019963, iteration: 4508\n",
      "cost: 1.013672, iteration: 4531\n",
      "cost: 1.007439, iteration: 4554\n",
      "cost: 1.001263, iteration: 4577\n",
      "cost: 0.995144, iteration: 4600\n",
      "cost: 0.989081, iteration: 4623\n",
      "cost: 0.983073, iteration: 4646\n",
      "cost: 0.977121, iteration: 4669\n",
      "cost: 0.971223, iteration: 4692\n",
      "cost: 0.965380, iteration: 4715\n",
      "cost: 0.959590, iteration: 4738\n",
      "cost: 0.953853, iteration: 4761\n",
      "cost: 0.948169, iteration: 4784\n",
      "cost: 0.942537, iteration: 4807\n",
      "cost: 0.936957, iteration: 4830\n",
      "cost: 0.931428, iteration: 4853\n",
      "cost: 0.925950, iteration: 4876\n",
      "cost: 0.920522, iteration: 4899\n",
      "cost: 0.915144, iteration: 4922\n",
      "cost: 0.909815, iteration: 4945\n",
      "cost: 0.904535, iteration: 4968\n",
      "cost: 0.899304, iteration: 4991\n",
      "cost: 0.894121, iteration: 5014\n",
      "cost: 0.888985, iteration: 5037\n",
      "cost: 0.883896, iteration: 5060\n",
      "cost: 0.878854, iteration: 5083\n",
      "cost: 0.873859, iteration: 5106\n",
      "cost: 0.868909, iteration: 5129\n",
      "cost: 0.864005, iteration: 5152\n",
      "cost: 0.859145, iteration: 5175\n",
      "cost: 0.854331, iteration: 5198\n",
      "cost: 0.849560, iteration: 5221\n",
      "cost: 0.844833, iteration: 5244\n",
      "cost: 0.840150, iteration: 5267\n",
      "cost: 0.835509, iteration: 5290\n",
      "cost: 0.830912, iteration: 5313\n",
      "cost: 0.826356, iteration: 5336\n",
      "cost: 0.821842, iteration: 5359\n",
      "cost: 0.817370, iteration: 5382\n",
      "cost: 0.812939, iteration: 5405\n",
      "cost: 0.808548, iteration: 5428\n",
      "cost: 0.804198, iteration: 5451\n",
      "cost: 0.799887, iteration: 5474\n",
      "cost: 0.795617, iteration: 5497\n",
      "cost: 0.791385, iteration: 5520\n",
      "cost: 0.787192, iteration: 5543\n",
      "cost: 0.783038, iteration: 5566\n",
      "cost: 0.778922, iteration: 5589\n",
      "cost: 0.774843, iteration: 5612\n",
      "cost: 0.770802, iteration: 5635\n",
      "cost: 0.766799, iteration: 5658\n",
      "cost: 0.762831, iteration: 5681\n",
      "cost: 0.758901, iteration: 5704\n",
      "cost: 0.755006, iteration: 5727\n",
      "cost: 0.751147, iteration: 5750\n",
      "cost: 0.747324, iteration: 5773\n",
      "cost: 0.743535, iteration: 5796\n",
      "cost: 0.739782, iteration: 5819\n",
      "cost: 0.736063, iteration: 5842\n",
      "cost: 0.732378, iteration: 5865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 0.725109, iteration: 5911\n",
      "cost: 0.717973, iteration: 5957\n",
      "cost: 0.710967, iteration: 6003\n",
      "cost: 0.704090, iteration: 6049\n",
      "cost: 0.697338, iteration: 6095\n",
      "cost: 0.690709, iteration: 6141\n",
      "cost: 0.684202, iteration: 6187\n",
      "cost: 0.677814, iteration: 6233\n",
      "cost: 0.671542, iteration: 6279\n",
      "cost: 0.665385, iteration: 6325\n",
      "cost: 0.659340, iteration: 6371\n",
      "cost: 0.653406, iteration: 6417\n",
      "cost: 0.647580, iteration: 6463\n",
      "cost: 0.641861, iteration: 6509\n",
      "cost: 0.636246, iteration: 6555\n",
      "cost: 0.630734, iteration: 6601\n",
      "cost: 0.625323, iteration: 6647\n",
      "cost: 0.620010, iteration: 6693\n",
      "cost: 0.614795, iteration: 6739\n",
      "cost: 0.609674, iteration: 6785\n",
      "cost: 0.604648, iteration: 6831\n",
      "cost: 0.599713, iteration: 6877\n",
      "cost: 0.594868, iteration: 6923\n",
      "cost: 0.590112, iteration: 6969\n",
      "cost: 0.585443, iteration: 7015\n",
      "cost: 0.580859, iteration: 7061\n",
      "cost: 0.576359, iteration: 7107\n",
      "cost: 0.571941, iteration: 7153\n",
      "cost: 0.567604, iteration: 7199\n",
      "cost: 0.563346, iteration: 7245\n",
      "cost: 0.559166, iteration: 7291\n",
      "cost: 0.555063, iteration: 7337\n",
      "cost: 0.551034, iteration: 7383\n",
      "cost: 0.547079, iteration: 7429\n",
      "cost: 0.543196, iteration: 7475\n",
      "cost: 0.539384, iteration: 7521\n",
      "cost: 0.535642, iteration: 7567\n",
      "cost: 0.531968, iteration: 7613\n",
      "cost: 0.528362, iteration: 7659\n",
      "cost: 0.524821, iteration: 7705\n",
      "cost: 0.521345, iteration: 7751\n",
      "cost: 0.517932, iteration: 7797\n",
      "cost: 0.514582, iteration: 7843\n",
      "cost: 0.511293, iteration: 7889\n",
      "cost: 0.508064, iteration: 7935\n",
      "cost: 0.504894, iteration: 7981\n",
      "cost: 0.501782, iteration: 8027\n",
      "cost: 0.498727, iteration: 8073\n",
      "cost: 0.495728, iteration: 8119\n",
      "cost: 0.492783, iteration: 8165\n",
      "cost: 0.489892, iteration: 8211\n",
      "cost: 0.487054, iteration: 8257\n",
      "cost: 0.484268, iteration: 8303\n",
      "cost: 0.481533, iteration: 8349\n",
      "cost: 0.478848, iteration: 8395\n",
      "cost: 0.476212, iteration: 8441\n",
      "cost: 0.473624, iteration: 8487\n",
      "cost: 0.471083, iteration: 8533\n",
      "cost: 0.468589, iteration: 8579\n",
      "cost: 0.466141, iteration: 8625\n",
      "cost: 0.463737, iteration: 8671\n",
      "cost: 0.461377, iteration: 8717\n",
      "cost: 0.459060, iteration: 8763\n",
      "cost: 0.455664, iteration: 8832\n",
      "cost: 0.452360, iteration: 8901\n",
      "cost: 0.449147, iteration: 8970\n",
      "cost: 0.446021, iteration: 9039\n",
      "cost: 0.442981, iteration: 9108\n",
      "cost: 0.440023, iteration: 9177\n",
      "cost: 0.437146, iteration: 9246\n",
      "cost: 0.434348, iteration: 9315\n",
      "cost: 0.431626, iteration: 9384\n",
      "cost: 0.428978, iteration: 9453\n",
      "cost: 0.426403, iteration: 9522\n",
      "cost: 0.423898, iteration: 9591\n",
      "cost: 0.421461, iteration: 9660\n",
      "cost: 0.419090, iteration: 9729\n",
      "cost: 0.416785, iteration: 9798\n",
      "cost: 0.414542, iteration: 9867\n",
      "cost: 0.412360, iteration: 9936\n",
      "cost: 0.410238, iteration: 10005\n",
      "cost: 0.408174, iteration: 10074\n",
      "cost: 0.405509, iteration: 10166\n",
      "cost: 0.402940, iteration: 10258\n",
      "cost: 0.400465, iteration: 10350\n",
      "cost: 0.398079, iteration: 10442\n",
      "cost: 0.395779, iteration: 10534\n",
      "cost: 0.393563, iteration: 10626\n",
      "cost: 0.391427, iteration: 10718\n",
      "cost: 0.389368, iteration: 10810\n",
      "cost: 0.387384, iteration: 10902\n",
      "cost: 0.385004, iteration: 11017\n",
      "cost: 0.382732, iteration: 11132\n",
      "cost: 0.380562, iteration: 11247\n",
      "cost: 0.378490, iteration: 11362\n",
      "cost: 0.376511, iteration: 11477\n",
      "cost: 0.374621, iteration: 11592\n",
      "cost: 0.372466, iteration: 11730\n",
      "cost: 0.370426, iteration: 11868\n",
      "cost: 0.368496, iteration: 12006\n",
      "cost: 0.366375, iteration: 12167\n",
      "cost: 0.364387, iteration: 12328\n",
      "cost: 0.362523, iteration: 12489\n",
      "cost: 0.360535, iteration: 12673\n",
      "cost: 0.358688, iteration: 12857\n",
      "cost: 0.356767, iteration: 13064\n",
      "cost: 0.354811, iteration: 13294\n",
      "cost: 0.353027, iteration: 13524\n",
      "cost: 0.351246, iteration: 13777\n",
      "cost: 0.349361, iteration: 14076\n",
      "cost: 0.347568, iteration: 14398\n",
      "cost: 0.345783, iteration: 14766\n",
      "cost: 0.343979, iteration: 15203\n",
      "cost: 0.342248, iteration: 15709\n",
      "cost: 0.340496, iteration: 16353\n",
      "cost: 0.338776, iteration: 17204\n",
      "cost: 0.337069, iteration: 18492\n",
      "cost: 0.335381, iteration: 21229\n",
      "running time: 102987.888122 seconds\n",
      "total iterations: 42458.000000\n",
      "C on training set:0.7895112476901152\n",
      "C on test set:0.7240666908300109\n",
      "\n",
      "FOLD 7:\n",
      "\n",
      "Using nesterov accelerated gradient\n",
      "training model\n",
      "cost: 4.499070, iteration: 0\n",
      "cost: 4.471881, iteration: 23\n",
      "cost: 4.434885, iteration: 46\n",
      "cost: 4.397338, iteration: 69\n",
      "cost: 4.360056, iteration: 92\n",
      "cost: 4.323109, iteration: 115\n",
      "cost: 4.286501, iteration: 138\n",
      "cost: 4.250228, iteration: 161\n",
      "cost: 4.214289, iteration: 184\n",
      "cost: 4.178679, iteration: 207\n",
      "cost: 4.143396, iteration: 230\n",
      "cost: 4.108437, iteration: 253\n",
      "cost: 4.073799, iteration: 276\n",
      "cost: 4.039479, iteration: 299\n",
      "cost: 4.005474, iteration: 322\n",
      "cost: 3.971780, iteration: 345\n",
      "cost: 3.938396, iteration: 368\n",
      "cost: 3.905319, iteration: 391\n",
      "cost: 3.872545, iteration: 414\n",
      "cost: 3.840072, iteration: 437\n",
      "cost: 3.807897, iteration: 460\n",
      "cost: 3.776017, iteration: 483\n",
      "cost: 3.744429, iteration: 506\n",
      "cost: 3.713132, iteration: 529\n",
      "cost: 3.682122, iteration: 552\n",
      "cost: 3.651396, iteration: 575\n",
      "cost: 3.620953, iteration: 598\n",
      "cost: 3.590789, iteration: 621\n",
      "cost: 3.560902, iteration: 644\n",
      "cost: 3.531289, iteration: 667\n",
      "cost: 3.501948, iteration: 690\n",
      "cost: 3.472876, iteration: 713\n",
      "cost: 3.444071, iteration: 736\n",
      "cost: 3.415530, iteration: 759\n",
      "cost: 3.387251, iteration: 782\n",
      "cost: 3.359232, iteration: 805\n",
      "cost: 3.331470, iteration: 828\n",
      "cost: 3.303963, iteration: 851\n",
      "cost: 3.276708, iteration: 874\n",
      "cost: 3.249704, iteration: 897\n",
      "cost: 3.222947, iteration: 920\n",
      "cost: 3.196436, iteration: 943\n",
      "cost: 3.170168, iteration: 966\n",
      "cost: 3.144141, iteration: 989\n",
      "cost: 3.118353, iteration: 1012\n",
      "cost: 3.092802, iteration: 1035\n",
      "cost: 3.067485, iteration: 1058\n",
      "cost: 3.042401, iteration: 1081\n",
      "cost: 3.017547, iteration: 1104\n",
      "cost: 2.992921, iteration: 1127\n",
      "cost: 2.968521, iteration: 1150\n",
      "cost: 2.944345, iteration: 1173\n",
      "cost: 2.920391, iteration: 1196\n",
      "cost: 2.896657, iteration: 1219\n",
      "cost: 2.873141, iteration: 1242\n",
      "cost: 2.849840, iteration: 1265\n",
      "cost: 2.826754, iteration: 1288\n",
      "cost: 2.803879, iteration: 1311\n",
      "cost: 2.781214, iteration: 1334\n",
      "cost: 2.758757, iteration: 1357\n",
      "cost: 2.736507, iteration: 1380\n",
      "cost: 2.714460, iteration: 1403\n",
      "cost: 2.692616, iteration: 1426\n",
      "cost: 2.670973, iteration: 1449\n",
      "cost: 2.649528, iteration: 1472\n",
      "cost: 2.628280, iteration: 1495\n",
      "cost: 2.607227, iteration: 1518\n",
      "cost: 2.586367, iteration: 1541\n",
      "cost: 2.565698, iteration: 1564\n",
      "cost: 2.545220, iteration: 1587\n",
      "cost: 2.524929, iteration: 1610\n",
      "cost: 2.504825, iteration: 1633\n",
      "cost: 2.484905, iteration: 1656\n",
      "cost: 2.465168, iteration: 1679\n",
      "cost: 2.445612, iteration: 1702\n",
      "cost: 2.426236, iteration: 1725\n",
      "cost: 2.407037, iteration: 1748\n",
      "cost: 2.388015, iteration: 1771\n",
      "cost: 2.369167, iteration: 1794\n",
      "cost: 2.350492, iteration: 1817\n",
      "cost: 2.331989, iteration: 1840\n",
      "cost: 2.313656, iteration: 1863\n",
      "cost: 2.295490, iteration: 1886\n",
      "cost: 2.277492, iteration: 1909\n",
      "cost: 2.259659, iteration: 1932\n",
      "cost: 2.241989, iteration: 1955\n",
      "cost: 2.224482, iteration: 1978\n",
      "cost: 2.207135, iteration: 2001\n",
      "cost: 2.189948, iteration: 2024\n",
      "cost: 2.172918, iteration: 2047\n",
      "cost: 2.156045, iteration: 2070\n",
      "cost: 2.139326, iteration: 2093\n",
      "cost: 2.122761, iteration: 2116\n",
      "cost: 2.106348, iteration: 2139\n",
      "cost: 2.090086, iteration: 2162\n",
      "cost: 2.073973, iteration: 2185\n",
      "cost: 2.058008, iteration: 2208\n",
      "cost: 2.042189, iteration: 2231\n",
      "cost: 2.026516, iteration: 2254\n",
      "cost: 2.010986, iteration: 2277\n",
      "cost: 1.995600, iteration: 2300\n",
      "cost: 1.980354, iteration: 2323\n",
      "cost: 1.965248, iteration: 2346\n",
      "cost: 1.950281, iteration: 2369\n",
      "cost: 1.935451, iteration: 2392\n",
      "cost: 1.920758, iteration: 2415\n",
      "cost: 1.906199, iteration: 2438\n",
      "cost: 1.891774, iteration: 2461\n",
      "cost: 1.877481, iteration: 2484\n",
      "cost: 1.863320, iteration: 2507\n",
      "cost: 1.849288, iteration: 2530\n",
      "cost: 1.835386, iteration: 2553\n",
      "cost: 1.821611, iteration: 2576\n",
      "cost: 1.807962, iteration: 2599\n",
      "cost: 1.794439, iteration: 2622\n",
      "cost: 1.781040, iteration: 2645\n",
      "cost: 1.767763, iteration: 2668\n",
      "cost: 1.754609, iteration: 2691\n",
      "cost: 1.741575, iteration: 2714\n",
      "cost: 1.728662, iteration: 2737\n",
      "cost: 1.715866, iteration: 2760\n",
      "cost: 1.703188, iteration: 2783\n",
      "cost: 1.690627, iteration: 2806\n",
      "cost: 1.678180, iteration: 2829\n",
      "cost: 1.665849, iteration: 2852\n",
      "cost: 1.653630, iteration: 2875\n",
      "cost: 1.641523, iteration: 2898\n",
      "cost: 1.629528, iteration: 2921\n",
      "cost: 1.617642, iteration: 2944\n",
      "cost: 1.605866, iteration: 2967\n",
      "cost: 1.594198, iteration: 2990\n",
      "cost: 1.582637, iteration: 3013\n",
      "cost: 1.571182, iteration: 3036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 1.559832, iteration: 3059\n",
      "cost: 1.548587, iteration: 3082\n",
      "cost: 1.537444, iteration: 3105\n",
      "cost: 1.526404, iteration: 3128\n",
      "cost: 1.515465, iteration: 3151\n",
      "cost: 1.504627, iteration: 3174\n",
      "cost: 1.493888, iteration: 3197\n",
      "cost: 1.483248, iteration: 3220\n",
      "cost: 1.472706, iteration: 3243\n",
      "cost: 1.462260, iteration: 3266\n",
      "cost: 1.451910, iteration: 3289\n",
      "cost: 1.441655, iteration: 3312\n",
      "cost: 1.431494, iteration: 3335\n",
      "cost: 1.421427, iteration: 3358\n",
      "cost: 1.411452, iteration: 3381\n",
      "cost: 1.401568, iteration: 3404\n",
      "cost: 1.391776, iteration: 3427\n",
      "cost: 1.382073, iteration: 3450\n",
      "cost: 1.372459, iteration: 3473\n",
      "cost: 1.362934, iteration: 3496\n",
      "cost: 1.353495, iteration: 3519\n",
      "cost: 1.344144, iteration: 3542\n",
      "cost: 1.334878, iteration: 3565\n",
      "cost: 1.325698, iteration: 3588\n",
      "cost: 1.316602, iteration: 3611\n",
      "cost: 1.307589, iteration: 3634\n",
      "cost: 1.298659, iteration: 3657\n",
      "cost: 1.289811, iteration: 3680\n",
      "cost: 1.281044, iteration: 3703\n",
      "cost: 1.272358, iteration: 3726\n",
      "cost: 1.263751, iteration: 3749\n",
      "cost: 1.255224, iteration: 3772\n",
      "cost: 1.246774, iteration: 3795\n",
      "cost: 1.238403, iteration: 3818\n",
      "cost: 1.230108, iteration: 3841\n",
      "cost: 1.221889, iteration: 3864\n",
      "cost: 1.213746, iteration: 3887\n",
      "cost: 1.205677, iteration: 3910\n",
      "cost: 1.197683, iteration: 3933\n",
      "cost: 1.189762, iteration: 3956\n",
      "cost: 1.181914, iteration: 3979\n",
      "cost: 1.174137, iteration: 4002\n",
      "cost: 1.166432, iteration: 4025\n",
      "cost: 1.158798, iteration: 4048\n",
      "cost: 1.151234, iteration: 4071\n",
      "cost: 1.143740, iteration: 4094\n",
      "cost: 1.136314, iteration: 4117\n",
      "cost: 1.128956, iteration: 4140\n",
      "cost: 1.121666, iteration: 4163\n",
      "cost: 1.114443, iteration: 4186\n",
      "cost: 1.107286, iteration: 4209\n",
      "cost: 1.100195, iteration: 4232\n",
      "cost: 1.093169, iteration: 4255\n",
      "cost: 1.086207, iteration: 4278\n",
      "cost: 1.079309, iteration: 4301\n",
      "cost: 1.072475, iteration: 4324\n",
      "cost: 1.065703, iteration: 4347\n",
      "cost: 1.058994, iteration: 4370\n",
      "cost: 1.052346, iteration: 4393\n",
      "cost: 1.045759, iteration: 4416\n",
      "cost: 1.039233, iteration: 4439\n",
      "cost: 1.032767, iteration: 4462\n",
      "cost: 1.026360, iteration: 4485\n",
      "cost: 1.020011, iteration: 4508\n",
      "cost: 1.013721, iteration: 4531\n",
      "cost: 1.007489, iteration: 4554\n",
      "cost: 1.001314, iteration: 4577\n",
      "cost: 0.995196, iteration: 4600\n",
      "cost: 0.989134, iteration: 4623\n",
      "cost: 0.983127, iteration: 4646\n",
      "cost: 0.977176, iteration: 4669\n",
      "cost: 0.971279, iteration: 4692\n",
      "cost: 0.965437, iteration: 4715\n",
      "cost: 0.959648, iteration: 4738\n",
      "cost: 0.953912, iteration: 4761\n",
      "cost: 0.948229, iteration: 4784\n",
      "cost: 0.942598, iteration: 4807\n",
      "cost: 0.937018, iteration: 4830\n",
      "cost: 0.931490, iteration: 4853\n",
      "cost: 0.926013, iteration: 4876\n",
      "cost: 0.920586, iteration: 4899\n",
      "cost: 0.915209, iteration: 4922\n",
      "cost: 0.909881, iteration: 4945\n",
      "cost: 0.904602, iteration: 4968\n",
      "cost: 0.899372, iteration: 4991\n",
      "cost: 0.894189, iteration: 5014\n",
      "cost: 0.889054, iteration: 5037\n",
      "cost: 0.883967, iteration: 5060\n",
      "cost: 0.878926, iteration: 5083\n",
      "cost: 0.873931, iteration: 5106\n",
      "cost: 0.868982, iteration: 5129\n",
      "cost: 0.864079, iteration: 5152\n",
      "cost: 0.859220, iteration: 5175\n",
      "cost: 0.854406, iteration: 5198\n",
      "cost: 0.849637, iteration: 5221\n",
      "cost: 0.844911, iteration: 5244\n",
      "cost: 0.840229, iteration: 5267\n",
      "cost: 0.835589, iteration: 5290\n",
      "cost: 0.830992, iteration: 5313\n",
      "cost: 0.826438, iteration: 5336\n",
      "cost: 0.821925, iteration: 5359\n",
      "cost: 0.817453, iteration: 5382\n",
      "cost: 0.813023, iteration: 5405\n",
      "cost: 0.808633, iteration: 5428\n",
      "cost: 0.804284, iteration: 5451\n",
      "cost: 0.799974, iteration: 5474\n",
      "cost: 0.795704, iteration: 5497\n",
      "cost: 0.791474, iteration: 5520\n",
      "cost: 0.787282, iteration: 5543\n",
      "cost: 0.783128, iteration: 5566\n",
      "cost: 0.779013, iteration: 5589\n",
      "cost: 0.774936, iteration: 5612\n",
      "cost: 0.770896, iteration: 5635\n",
      "cost: 0.766893, iteration: 5658\n",
      "cost: 0.762927, iteration: 5681\n",
      "cost: 0.758997, iteration: 5704\n",
      "cost: 0.755103, iteration: 5727\n",
      "cost: 0.751245, iteration: 5750\n",
      "cost: 0.747423, iteration: 5773\n",
      "cost: 0.743635, iteration: 5796\n",
      "cost: 0.739882, iteration: 5819\n",
      "cost: 0.736164, iteration: 5842\n",
      "cost: 0.732480, iteration: 5865\n",
      "cost: 0.725213, iteration: 5911\n",
      "cost: 0.718079, iteration: 5957\n",
      "cost: 0.711075, iteration: 6003\n",
      "cost: 0.704199, iteration: 6049\n",
      "cost: 0.697449, iteration: 6095\n",
      "cost: 0.690822, iteration: 6141\n",
      "cost: 0.684317, iteration: 6187\n",
      "cost: 0.677930, iteration: 6233\n",
      "cost: 0.671660, iteration: 6279\n",
      "cost: 0.665504, iteration: 6325\n",
      "cost: 0.659462, iteration: 6371\n",
      "cost: 0.653529, iteration: 6417\n",
      "cost: 0.647705, iteration: 6463\n",
      "cost: 0.641987, iteration: 6509\n",
      "cost: 0.636374, iteration: 6555\n",
      "cost: 0.630864, iteration: 6601\n",
      "cost: 0.625454, iteration: 6647\n",
      "cost: 0.620143, iteration: 6693\n",
      "cost: 0.614929, iteration: 6739\n",
      "cost: 0.609811, iteration: 6785\n",
      "cost: 0.604786, iteration: 6831\n",
      "cost: 0.599852, iteration: 6877\n",
      "cost: 0.595009, iteration: 6923\n",
      "cost: 0.590255, iteration: 6969\n",
      "cost: 0.585587, iteration: 7015\n",
      "cost: 0.581005, iteration: 7061\n",
      "cost: 0.576506, iteration: 7107\n",
      "cost: 0.572090, iteration: 7153\n",
      "cost: 0.567754, iteration: 7199\n",
      "cost: 0.563498, iteration: 7245\n",
      "cost: 0.559320, iteration: 7291\n",
      "cost: 0.555217, iteration: 7337\n",
      "cost: 0.551190, iteration: 7383\n",
      "cost: 0.547236, iteration: 7429\n",
      "cost: 0.543355, iteration: 7475\n",
      "cost: 0.539545, iteration: 7521\n",
      "cost: 0.535804, iteration: 7567\n",
      "cost: 0.532131, iteration: 7613\n",
      "cost: 0.528526, iteration: 7659\n",
      "cost: 0.524987, iteration: 7705\n",
      "cost: 0.521512, iteration: 7751\n",
      "cost: 0.518101, iteration: 7797\n",
      "cost: 0.514752, iteration: 7843\n",
      "cost: 0.511464, iteration: 7889\n",
      "cost: 0.508236, iteration: 7935\n",
      "cost: 0.505068, iteration: 7981\n",
      "cost: 0.501957, iteration: 8027\n",
      "cost: 0.498903, iteration: 8073\n",
      "cost: 0.495905, iteration: 8119\n",
      "cost: 0.492962, iteration: 8165\n",
      "cost: 0.490073, iteration: 8211\n",
      "cost: 0.487236, iteration: 8257\n",
      "cost: 0.484451, iteration: 8303\n",
      "cost: 0.481717, iteration: 8349\n",
      "cost: 0.479033, iteration: 8395\n",
      "cost: 0.476398, iteration: 8441\n",
      "cost: 0.473812, iteration: 8487\n",
      "cost: 0.471272, iteration: 8533\n",
      "cost: 0.468779, iteration: 8579\n",
      "cost: 0.466331, iteration: 8625\n",
      "cost: 0.463929, iteration: 8671\n",
      "cost: 0.461570, iteration: 8717\n",
      "cost: 0.459254, iteration: 8763\n",
      "cost: 0.455859, iteration: 8832\n",
      "cost: 0.452558, iteration: 8901\n",
      "cost: 0.449346, iteration: 8970\n",
      "cost: 0.446222, iteration: 9039\n",
      "cost: 0.443183, iteration: 9108\n",
      "cost: 0.440227, iteration: 9177\n",
      "cost: 0.437351, iteration: 9246\n",
      "cost: 0.434554, iteration: 9315\n",
      "cost: 0.431834, iteration: 9384\n",
      "cost: 0.429188, iteration: 9453\n",
      "cost: 0.426613, iteration: 9522\n",
      "cost: 0.424109, iteration: 9591\n",
      "cost: 0.421674, iteration: 9660\n",
      "cost: 0.419305, iteration: 9729\n",
      "cost: 0.417000, iteration: 9798\n",
      "cost: 0.414759, iteration: 9867\n",
      "cost: 0.412578, iteration: 9936\n",
      "cost: 0.410457, iteration: 10005\n",
      "cost: 0.408394, iteration: 10074\n",
      "cost: 0.405731, iteration: 10166\n",
      "cost: 0.403164, iteration: 10258\n",
      "cost: 0.400689, iteration: 10350\n",
      "cost: 0.398305, iteration: 10442\n",
      "cost: 0.396007, iteration: 10534\n",
      "cost: 0.393792, iteration: 10626\n",
      "cost: 0.391657, iteration: 10718\n",
      "cost: 0.389600, iteration: 10810\n",
      "cost: 0.387617, iteration: 10902\n",
      "cost: 0.385239, iteration: 11017\n",
      "cost: 0.382968, iteration: 11132\n",
      "cost: 0.380799, iteration: 11247\n",
      "cost: 0.378728, iteration: 11362\n",
      "cost: 0.376750, iteration: 11477\n",
      "cost: 0.374862, iteration: 11592\n",
      "cost: 0.372708, iteration: 11730\n",
      "cost: 0.370669, iteration: 11868\n",
      "cost: 0.368741, iteration: 12006\n",
      "cost: 0.366621, iteration: 12167\n",
      "cost: 0.364634, iteration: 12328\n",
      "cost: 0.362771, iteration: 12489\n",
      "cost: 0.360785, iteration: 12673\n",
      "cost: 0.358939, iteration: 12857\n",
      "cost: 0.357019, iteration: 13064\n",
      "cost: 0.355064, iteration: 13294\n",
      "cost: 0.353282, iteration: 13524\n",
      "cost: 0.351502, iteration: 13777\n",
      "cost: 0.349618, iteration: 14076\n",
      "cost: 0.347826, iteration: 14398\n",
      "cost: 0.346042, iteration: 14766\n",
      "cost: 0.344239, iteration: 15203\n",
      "cost: 0.342509, iteration: 15709\n",
      "cost: 0.340758, iteration: 16353\n",
      "cost: 0.339039, iteration: 17204\n",
      "cost: 0.337331, iteration: 18492\n",
      "cost: 0.335642, iteration: 21229\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "<class 'range'> returned a result with an error set\nApply node that caused the error: Elemwise{Composite{tanh((i0 + i1))}}[(0, 0)](Dot22.0, InplaceDimShuffle{x,0}.0)\nToposort index: 8\nInputs types: [TensorType(float64, matrix), TensorType(float64, row)]\nInputs shapes: [(3360, 22), (1, 22)]\nInputs strides: [(176, 8), (176, 8)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[Dot22(Elemwise{Composite{tanh((i0 + i1))}}[(0, 0)].0, W_cox)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\theano\\scalar\\basic.py\u001b[0m in \u001b[0;36mimpl\u001b[1;34m(self, *inputs)\u001b[0m\n\u001b[0;32m   4021\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mimpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4022\u001b[1;33m         \u001b[0moutput_storage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4023\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_storage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\theano\\gof\\vm.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    300\u001b[0m                                                     self.post_thunk_clear):\n\u001b[1;32m--> 301\u001b[1;33m                     \u001b[0mthunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mold_s\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mold_storage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\theano\\gof\\op.py\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    891\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 892\u001b[1;33m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    893\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\theano\\tensor\\elemwise.py\u001b[0m in \u001b[0;36mperform\u001b[1;34m(self, node, inputs, output_storage)\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 790\u001b[1;33m         \u001b[0mvariables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mufunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mufunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    791\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\theano\\scalar\\basic.py\u001b[0m in \u001b[0;36mimpl\u001b[1;34m(self, *inputs)\u001b[0m\n\u001b[0;32m   4021\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mimpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4022\u001b[1;33m         \u001b[0moutput_storage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4023\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_storage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: <class 'range'> returned a result with an error set",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-6e89fd14bb3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mfeaturespace_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'event'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mmodel_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mL2_reg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     cox_nnet_model, cox_nnet_cost_iter = cox_nnet.trainCoxMlp(featurespace_train\n\u001b[0m\u001b[0;32m     38\u001b[0m                                                               \u001b[1;33m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                                                               \u001b[1;33m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mU:\\Hieu\\CARDIA_project\\CARDIA_project\\cvd_outcome_rerun_2\\cardia_rerun_2_code\\snippet\\cox_nnet.py\u001b[0m in \u001b[0;36mtrainCoxMlp\u001b[1;34m(x_train, ytime_train, ystatus_train, model_params, search_params, verbose)\u001b[0m\n\u001b[0;32m    371\u001b[0m         \u001b[1;31m# if method == \"momentum\" or method == \"gradient\":\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0meval_step\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m             \u001b[0mcost_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcost_iter\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbest_cost\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m                 \u001b[0mbest_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcost_iter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\theano\\gof\\graph.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, inputs_to_values)\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minputs_to_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m         \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    901\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 903\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    905\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\theano\\gof\\vm.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    303\u001b[0m                         \u001b[0mold_s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m                 \u001b[0mlink\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_with_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\theano\\gof\\link.py\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    700\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\theano\\gof\\vm.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    299\u001b[0m                 for thunk, node, old_storage in zip(self.thunks, self.nodes,\n\u001b[0;32m    300\u001b[0m                                                     self.post_thunk_clear):\n\u001b[1;32m--> 301\u001b[1;33m                     \u001b[0mthunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mold_s\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mold_storage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m                         \u001b[0mold_s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\theano\\gof\\op.py\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    890\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 892\u001b[1;33m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    893\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\theano\\tensor\\elemwise.py\u001b[0m in \u001b[0;36mperform\u001b[1;34m(self, node, inputs, output_storage)\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[0mnout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 790\u001b[1;33m         \u001b[0mvariables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mufunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mufunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    791\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnout\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\theano\\scalar\\basic.py\u001b[0m in \u001b[0;36mimpl\u001b[1;34m(self, *inputs)\u001b[0m\n\u001b[0;32m   4020\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4021\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mimpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4022\u001b[1;33m         \u001b[0moutput_storage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4023\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_storage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4024\u001b[0m         ret = utils.to_return_values([storage[0] for storage in\n",
      "\u001b[1;31mSystemError\u001b[0m: <class 'range'> returned a result with an error set\nApply node that caused the error: Elemwise{Composite{tanh((i0 + i1))}}[(0, 0)](Dot22.0, InplaceDimShuffle{x,0}.0)\nToposort index: 8\nInputs types: [TensorType(float64, matrix), TensorType(float64, row)]\nInputs shapes: [(3360, 22), (1, 22)]\nInputs strides: [(176, 8), (176, 8)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[Dot22(Elemwise{Composite{tanh((i0 + i1))}}[(0, 0)].0, W_cox)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "# Outerloop:\n",
    "for fold in range(6,25):\n",
    "\n",
    "    #fold = 24\n",
    "\n",
    "    print('')\n",
    "    print('FOLD '+str(fold)+':')\n",
    "    print('')\n",
    "\n",
    "    trainingid = trainingid_all.iloc[:,fold]\n",
    "    trainingid = trainingid[~np.isnan(trainingid)]\n",
    "    eligible_id = data_full['ID'][data_full['ID'].isin(trainingid)]\n",
    "    train_df = data_full.loc[data_full['ID'].isin(eligible_id),:]\n",
    "    del train_df['ID']\n",
    "    test_df = data_full.loc[~data_full['ID'].isin(eligible_id),:]\n",
    "    del test_df['ID']\n",
    "\n",
    "\n",
    "    model_params = dict(node_map = None, input_split = None)\n",
    "    search_params = dict(method = \"nesterov\"\n",
    "                         , learning_rate=0.0001\n",
    "                         , momentum=0.9\n",
    "                         , max_iter=200000\n",
    "                         , stop_threshold=0.995\n",
    "                         , patience=1000\n",
    "                         , patience_incr=2\n",
    "                         , rand_seed = 9114\n",
    "                         , eval_step=23\n",
    "                         , lr_decay = 0.9\n",
    "                         , lr_growth = 1.0)\n",
    "\n",
    "    # #train final model\n",
    "    L2_reg = 0.1\n",
    "    featurespace_train = train_df.drop(['time','event'],axis =1).values\n",
    "    featurespace_test = test_df.drop(['time','event'],axis =1).values\n",
    "    model_params = dict(node_map = None, input_split = None, L2_reg=0.1)\n",
    "    cox_nnet_model, cox_nnet_cost_iter = cox_nnet.trainCoxMlp(featurespace_train\n",
    "                                                              , train_df.time.values\n",
    "                                                              , train_df.event.values\n",
    "                                                              , model_params\n",
    "                                                              , search_params\n",
    "                                                              , verbose=True)\n",
    "    cox_nnet_theta_train = cox_nnet_model.predictNewData(featurespace_train)\n",
    "    cox_nnet_theta_test = cox_nnet_model.predictNewData(featurespace_test)\n",
    "\n",
    "    #discrimination on train, test sets\n",
    "    c_train = concordance_index(train_df.time,-cox_nnet_theta_train, train_df.event)\n",
    "    print('C on training set:' + str(c_train))\n",
    "    c_test = concordance_index(test_df.time,-cox_nnet_theta_test, test_df.event)\n",
    "    print('C on test set:' + str(c_test))\n",
    "\n",
    "    predicted_risk = cox_nnet_theta_test\n",
    "    predicted_risk = pd.DataFrame(data=predicted_risk)\n",
    "    times, H0 = cox_basehaz(cox_nnet_theta_train.astype('float32').flatten(), train_df.time.values, train_df.event.values.astype(bool))\n",
    "    trained_time = pd.DataFrame(data = times)\n",
    "    pred_prob_surv = cox_pred_surv(cox_nnet_theta_test.astype('float32').flatten(), H0)\n",
    "    pred_prob_surv = pd.DataFrame(data=pred_prob_surv)\n",
    "\n",
    "\n",
    "    savedir = os.path.join(work_dir,'csv_files/cox_nnet/'+str(n_features)+'_features/'+term_pred)\n",
    "    try: \n",
    "        os.makedirs(savedir)\n",
    "    except OSError:\n",
    "        if not os.path.isdir(savedir):\n",
    "            raise\n",
    "    actual_fold = fold+1\n",
    "\n",
    "    predicted_risk.to_csv(savedir+'/lp_fold_'+str(actual_fold)+'.csv', index = None, header = True)\n",
    "    pred_prob_surv.to_csv(savedir+'/pred_prob_surv_fold_'+str(actual_fold)+'.csv', index = None, header = True)\n",
    "    trained_time.to_csv(savedir+'/trained_time_fold_'+str(actual_fold)+'.csv', index = None, header = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find conda environment: tf-gpu\n",
      "You can list all discoverable environments with `conda info --envs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-gpu in c:\\programdata\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\hnguye78\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow-gpu) (1.12.1)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.12.0)\n",
      "Requirement already satisfied: tensorboard~=2.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (2.5.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.19.2)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.35.1)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (3.7.4.3)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.32.0)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\users\\hnguye78\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow-gpu) (0.3.3)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.1.2)\n",
      "Requirement already satisfied: h5py~=2.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (2.10.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.6.3)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (3.15.8)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (2.4.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\hnguye78\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow-gpu) (3.3.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.2.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.1.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\hnguye78\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow-gpu) (1.12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for backports-tempfile: [Errno 13] Permission denied: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\backports.tempfile-1.0.dist-info\\\\METADATA'\n",
      "WARNING: Error parsing requirements for backports-functools-lru-cache: [Errno 13] Permission denied: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\backports.functools_lru_cache-1.6.1.dist-info\\\\METADATA'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: six~=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.15.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (2.24.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (50.3.1.post20201107)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (1.30.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (0.4.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (0.6.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (1.8.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (2.10)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (1.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "# !Conda create -n tf-gpu\n",
    "# !Conda activate tf-gpu\n",
    "# !pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['THEANO_FLAGS']= 'mode=FAST_RUN, device=cuda0, floatX=float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elemwise{exp,no_inplace}(<TensorType(float64, vector)>)]\n",
      "Looping 1000 times took 2.456049 seconds\n",
      "Result is [1.23178032 1.61879341 1.52278065 ... 2.20771815 2.29967753 1.62323285]\n",
      "Used the cpu\n"
     ]
    }
   ],
   "source": [
    "from theano import function, config, shared, tensor as tt\n",
    "import numpy\n",
    "import time\n",
    "\n",
    "vlen = 10 * 30 * 768  # 10 x #cores x # threads per core\n",
    "iters = 1000\n",
    "\n",
    "rng = numpy.random.RandomState(22)\n",
    "x = shared(numpy.asarray(rng.rand(vlen), config.floatX))\n",
    "f = function([], tt.exp(x))\n",
    "print(f.maker.fgraph.toposort())\n",
    "t0 = time.time()\n",
    "for i in range(iters):\n",
    "    r = f()\n",
    "t1 = time.time()\n",
    "print(\"Looping %d times took %f seconds\" % (iters, t1 - t0))\n",
    "print(\"Result is %s\" % (r,))\n",
    "if numpy.any([isinstance(x.op, theano.tensor.elemwise.Elemwise) and\n",
    "              ('Gpu' not in type(x.op).__name__)\n",
    "              for x in f.maker.fgraph.toposort()]):\n",
    "    print('Used the cpu')\n",
    "else:\n",
    "    print('Used the gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "SkipTest",
     "evalue": "You are importing theano.sandbox.cuda. This is the old GPU back-end and is removed from Theano. Use Theano 0.9 to use it. Even better, transition to the new GPU back-end! See https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSkipTest\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-54b4f6bd6094>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msandbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msandbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'gpu0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\theano\\sandbox\\cuda\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# thus nosetests will look for test files into this folder. With a SkipTest raised,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# the folder will be skipped by nosetests without failing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m raise SkipTest(\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;34m\"You are importing theano.sandbox.cuda. This is the old GPU back-end and \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;34m\"is removed from Theano. Use Theano 0.9 to use it. Even better, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSkipTest\u001b[0m: You are importing theano.sandbox.cuda. This is the old GPU back-end and is removed from Theano. Use Theano 0.9 to use it. Even better, transition to the new GPU back-end! See https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29"
     ]
    }
   ],
   "source": [
    "import theano.sandbox.cuda\n",
    "theano.sandbox.cuda.use('gpu0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'system' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-1664578f0362>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mPARAMS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'THEANO_FLAGS=mode=FAT_RUN.device=gpu0, floatX=float32 %s'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPARAM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cox_nnet_long_term_prediction_all_var.ipynb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'system' is not defined"
     ]
    }
   ],
   "source": [
    "PARAMS = 'THEANO_FLAGS=mode=FAT_RUN.device=gpu0, floatX=float32 %s'\n",
    "system(sprint(PARAM, 'cox_nnet_long_term_prediction_all_var.ipynb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "tf_device = '/GPU:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17748565743607172743\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 31753514624\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      device_id: 1\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 16507535774320947291\n",
      "physical_device_desc: \"device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:25:00.0, compute capability: 7.0\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 31753514624\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 2618526900162166560\n",
      "physical_device_desc: \"device: 1, name: Tesla V100-PCIE-32GB, pci bus id: 0000:c8:00.0, compute capability: 7.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cross validation:\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# n_folds = 10\n",
    "# kf=StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=0)\n",
    "# early_stopping = EarlyStopping(monitor='loss', patience=20)\n",
    "\n",
    "# #l2_array = np.concatenate(([0.],np.power(10.,np.arange(-6,-2))))\n",
    "# l2_array = np.power(10.,np.arange(-3,2))\n",
    "# grid_search_train = np.zeros((len(l2_array),n_folds))\n",
    "# grid_search_test = np.zeros((len(l2_array),n_folds))\n",
    "# grid_search_c_index_train = np.zeros((len(l2_array),n_folds))\n",
    "# grid_search_c_index_test = np.zeros((len(l2_array),n_folds))\n",
    "\n",
    "# for i in range(len(l2_array)):\n",
    "#     #i = 0\n",
    "#     j=0\n",
    "#     cv_folds = kf.split(train_df.values, train_df[['event']].values)\n",
    "#     for traincv, testcv in cv_folds:\n",
    "#         print('i in len(l2_array)=' + str(i+1) + '/' + str(len(l2_array)))\n",
    "#         print('cv fold =' + str(j+1) + '/' + str(n_folds))\n",
    "#         x_train_cv = featurespace_train_df.iloc[traincv].values\n",
    "#         y_train_cv = y_train[traincv]\n",
    "#         x_test_cv = featurespace_train_df.iloc[testcv].values\n",
    "#         y_test_cv = y_train[testcv]\n",
    "\n",
    "#         model = Sequential()\n",
    "#         #model.add(Dense(n_intervals,input_dim=x_train.shape[1],bias_initializer='zeros',kernel_regularizer=regularizers.l2(l2_array[i])))\n",
    "#         model.add(Dense(hidden_layers_sizes\n",
    "#                   , input_dim=featurespace_train.shape[1]\n",
    "#                   , bias_initializer='zeros'\n",
    "#                   , activation='relu'\n",
    "#                   , kernel_regularizer=regularizers.l2(l2_array[i])\n",
    "#                   ))\n",
    "#         model.add(Dense(n_intervals))\n",
    "#         model.add(Activation('sigmoid'))\n",
    "#         model.compile(loss=nnet_survival.surv_likelihood(n_intervals)\n",
    "#                       , optimizer=optimizers.Adam()) #lr=0.0001))\n",
    "#         history=model.fit(x_train_cv, y_train_cv\n",
    "#                     , batch_size=256\n",
    "#                     , epochs=1000\n",
    "#                     , callbacks=[early_stopping]\n",
    "#                     , verbose=1\n",
    "#                     , validation_data=(x_test_cv, y_test_cv))\n",
    "#         grid_search_train[i,j] = model.evaluate(x_train_cv,y_train_cv,verbose=0)\n",
    "#         grid_search_test[i,j] = model.evaluate(x_test_cv,y_test_cv,verbose=0)\n",
    "              \n",
    "#         #Discrimination performance\n",
    "#         # y_pred is conditional prob of survival within each time interval\n",
    "#         y_pred_train_cv = model.predict_proba(x_train_cv ,verbose=0)\n",
    "#         yr26_surv_train_cv = nnet_pred_surv(y_pred_train_cv, breaks, 365.25*26)\n",
    "#         y_pred_test_cv=model.predict_proba(x_test_cv ,verbose=0)\n",
    "#         yr26_surv_test_cv = nnet_pred_surv(y_pred_test_cv, breaks, 365.25*26)\n",
    "        \n",
    "#         grid_search_c_index_train[i,j] = concordance_index(train_df.time.iloc[traincv], yr26_surv_train_cv, train_df.event.iloc[traincv])\n",
    "#         grid_search_c_index_test[i,j] = concordance_index(train_df.time.iloc[testcv], yr26_surv_test_cv, train_df.event.iloc[testcv])\n",
    "        \n",
    "#         print(' ')\n",
    "#         print('Looking at  i=' + str(i+1) + '/' + str(len(l2_array)) + (' and cv fold =' + str(j+1) + '/' + str(n_folds)))      \n",
    "#         print('Loss Train, Loss Valid, C-index Train, C-index Test:')\n",
    "#         print(grid_search_train[i,j])\n",
    "#         print(grid_search_test[i,j])\n",
    "#         print(grid_search_c_index_train[i,j])\n",
    "#         print(grid_search_c_index_test[i,j]) \n",
    "#         print(' ')\n",
    "              \n",
    "#         j=j+1\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "# print('grid_search_train:')\n",
    "# print(grid_search_train)\n",
    "# print('grid_search_test:')\n",
    "# print(grid_search_test)\n",
    "# print(np.average(grid_search_train,axis=1))\n",
    "# print(np.average(grid_search_test,axis=1))\n",
    "# print(grid_search_c_index_train)\n",
    "# print(grid_search_c_index_test)\n",
    "#\n",
    "#l2_final = l2_array[np.argmax(-np.average(grid_search_test,axis=1))]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ##########################\n",
    "# # Build model with tuned hyperparam:\n",
    "# l2_final=0.0001\n",
    "\n",
    "# from numpy.random import seed\n",
    "\n",
    "# seed(1)\n",
    "# import tensorflow as tf\n",
    "# import keras\n",
    "# tf.random.set_seed(2)\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(hidden_layers_sizes\n",
    "#                 , input_dim=featurespace_train.shape[1]\n",
    "#                 , bias_initializer='zeros'\n",
    "#                 , kernel_regularizer=regularizers.l2(l2_final)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(n_intervals))\n",
    "# model.add(Activation('sigmoid'))\n",
    "# # import keras.backend.tensorflow_backend as kk\n",
    "\n",
    "# model.compile(loss=nnet_survival.surv_likelihood(n_intervals), optimizer=optimizers.RMSprop())\n",
    "# early_stopping = EarlyStopping(monitor='loss', patience=20)\n",
    "# history=model.fit(featurespace_train, y_train, batch_size=4063, epochs=100000\n",
    "#                   , callbacks=[early_stopping]\n",
    "#                   , verbose=0)\n",
    "\n",
    "\n",
    "# # print loss of train and valid data:\n",
    "# # print(model.evaluate(featurespace_train,y_train,verbose=0))\n",
    "# # model.evaluate(featurespace_test,y_test,verbose=0)\n",
    "\n",
    "# #Discrimination performance\n",
    "# # y_pred is conditional prob of survival within each time interval\n",
    "# y_pred_train = model.predict(featurespace_train,verbose=0)\n",
    "# # cumprod = cumulative product, the probability of surviving from time 0 up to the time interval of interest\n",
    "# # index -1 because we are interested in -1 \n",
    "# last_yr_surv_train=np.cumprod(y_pred_train[:,0:np.nonzero(breaks>365*(endpt-1))[0][0]], axis=1)[:,-1]\n",
    "# print('Train C-index fold', str(fold+1),':')\n",
    "# print(concordance_index(train_df.time, last_yr_surv_train, train_df.event)) \n",
    "\n",
    "# y_pred=model.predict(featurespace_test,verbose=0)\n",
    "# last_yr_surv=np.cumprod(y_pred[:,0:np.nonzero(breaks>365*(endpt-1))[0][0]], axis=1)[:,-1]\n",
    "# print('Test C-index fold', str(fold+1),':')\n",
    "# print(concordance_index(test_df.time,last_yr_surv, test_df.event))\n",
    "\n",
    "# pred_surv = np.zeros((len(test_df.event), len(eval_times)))\n",
    "# col=0\n",
    "# for time in eval_times:\n",
    "#     pred_surv[:,col] = nnet_pred_surv(y_pred, breaks, time)\n",
    "#     col = col+1\n",
    "# pred_surv = pd.DataFrame(data = pred_surv)\n",
    "# # savedir = os.path.join(os.getcwd(),'python_files/csv_files/nnet_survival/all_features')\n",
    "# savedir = os.path.join(work_dir,'csv_files/nnet_survival/'+str(n_features)+'_features/'+term_pred)\n",
    "# try: \n",
    "#     os.makedirs(savedir)\n",
    "# except OSError:\n",
    "#     if not os.path.isdir(savedir):\n",
    "#         raise\n",
    "# actual_fold = fold+1\n",
    "# pred_surv.to_csv(savedir+'/pred_prob_surv_fold_'+str(actual_fold)+'.csv', index = None, header = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
