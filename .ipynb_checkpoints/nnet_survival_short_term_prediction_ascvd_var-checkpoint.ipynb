{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_features = 'ascvd'\n",
    "term_pred = 'short_term'\n",
    "data_file_name = 'data_y10_ascvd'\n",
    "endpt = 10\n",
    "eval_times = 365.25*np.r_[1, np.arange(2, endpt+1, 2)]\n",
    "work_dir = '/home/idies/workspace/Storage/hnguye78/persistent/CARDIA_project/cvd_outcome_rerun2'\n",
    "work_dir = 'U:/Hieu/CARDIA_project/CARDIA_project/cvd_outcome_rerun_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow\n",
    "# %pip install lifelines\n",
    "\n",
    "#!y | pip uninstall statsmodels \n",
    "# %pip install statsmodels==0.11.0\n",
    "\n",
    "# %pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, LSTM, GRU, Embedding, Concatenate, Conv1D, GlobalMaxPooling1D, MaxPooling1D, GlobalAveragePooling1D, BatchNormalization, TimeDistributed\n",
    "from tensorflow.keras import optimizers, layers, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "import math\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import concordance_index\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#snippets_dir = '/home/idies/workspace/Storage/hnguye78/persistent/CARDIA_project/cvd_outcome_rerun'+ '/code/snippet'\n",
    "snippets_dir = 'U:/Hieu/CARDIA_project/CARDIA_project/Git/Python_code/snippets'\n",
    "import os, sys\n",
    "sys.path.append(os.path.abspath(snippets_dir))\n",
    "import nnet_survival\n",
    "from cox import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dir = work_dir+ '/csv_files'\n",
    "\n",
    "# load data:\n",
    "data_full = pd.read_csv(load_dir+'/'+data_file_name+'.csv')\n",
    "data_full = data_full.select_dtypes(include =[np.number])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training id:\n",
    "loaddir = work_dir+ '/csv_files'\n",
    "trainingid_all = pd.read_csv(loaddir+'/all_training_ID_outerloop_cohort_0_10.csv')\n",
    "\n",
    "## standardize feature space, then merge back to label space:\n",
    "feature_space = data_full.drop(['ID','event','time'], axis = 1)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(feature_space)\n",
    "scaled_feature_space = scaler.transform(feature_space)\n",
    "scaled_feature_space_df = pd.DataFrame(data=scaled_feature_space[0:,0:])\n",
    "scaled_feature_space_df.insert(0, 'ID', data_full['ID'], True)\n",
    "label = data_full.loc[:,['ID','event','time']]\n",
    "data_full = pd.merge(label, scaled_feature_space_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = data_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide on number of dicrete times:\n",
    "#halflife=13.*365.25\n",
    "\n",
    "#breaks = 365.25*np.r_[1, np.arange(2, endpt+1, 0.5)]\n",
    "breaks = 365.25*np.arange(1, endpt,0.5)\n",
    "#breaks=-np.log(1-np.arange(0.0,0.96,0.05))*halflife/np.log(2) \n",
    "n_intervals=len(breaks)-1\n",
    "timegap = breaks[1:] - breaks[:-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize hyperparam:\n",
    "hidden_layers_sizes = 4\n",
    "n_epochs = 50000\n",
    "learning_rate = 0.001\n",
    "lr_decay = 0.001\n",
    "momentum = 0.9\n",
    "n_in = data_full.shape[1]-3 # number of features\n",
    "\n",
    "\n",
    "def nnet_pred_surv(y_pred, breaks, fu_time):\n",
    "#Predicted survival probability from Nnet-survival model\n",
    "#Inputs are Numpy arrays.\n",
    "#y_pred: Rectangular array, each individual's conditional probability of surviving each time interval\n",
    "#breaks: Break-points for time intervals used for Nnet-survival model, starting with 0\n",
    "#fu_time: Follow-up time point at which predictions are needed\n",
    "#\n",
    "#Returns: predicted survival probability for each individual at specified follow-up time\n",
    "  y_pred=np.cumprod(y_pred, axis=1)\n",
    "  pred_surv = []\n",
    "  for i in range(y_pred.shape[0]):\n",
    "    pred_surv.append(np.interp(fu_time,breaks[1:],y_pred[i,:]))\n",
    "  return np.array(pred_surv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outerloop:\n",
    "for fold in range(25):\n",
    "\n",
    "    ## fold = 24\n",
    "    print('')\n",
    "    print('FOLD '+str(fold)+':')\n",
    "    print('')\n",
    "\n",
    "    trainingid = trainingid_all.iloc[:,fold]\n",
    "    trainingid = trainingid[~np.isnan(trainingid)]\n",
    "    eligible_id = data_full['ID'][data_full['ID'].isin(trainingid)]\n",
    "    train_df = data_full.loc[data_full['ID'].isin(eligible_id),:]\n",
    "    del train_df['ID']\n",
    "    test_df = data_full.loc[~data_full['ID'].isin(eligible_id),:]\n",
    "    del test_df['ID']\n",
    "\n",
    "\n",
    "    # reformat the train and test set:\n",
    "    y_train = nnet_survival.make_surv_array(train_df.time.values\n",
    "                                            , train_df.event.values\n",
    "                                            , breaks)\n",
    "    y_test = nnet_survival.make_surv_array(test_df.time.values\n",
    "                                           , test_df.event.values\n",
    "                                           , breaks)\n",
    "    featurespace_train_df = train_df.drop(['time','event'],axis =1)\n",
    "    featurespace_test_df = test_df.drop(['time','event'],axis =1)\n",
    "\n",
    "    featurespace_train = train_df.drop(['time','event'],axis =1).values\n",
    "    featurespace_test = test_df.drop(['time','event'],axis =1).values\n",
    "\n",
    "\n",
    "\n",
    "    ##########################\n",
    "    # Build model:\n",
    "    # l2_final=0.0001\n",
    "    l2_final=0.1\n",
    "    from numpy.random import seed\n",
    "\n",
    "    seed(1)\n",
    "    import tensorflow as tf\n",
    "    import keras\n",
    "    tf.random.set_seed(2)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layers_sizes\n",
    "                    , input_dim=featurespace_train.shape[1]\n",
    "                    , bias_initializer='zeros'\n",
    "                    , kernel_regularizer=regularizers.l2(l2_final)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(n_intervals))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    # import keras.backend.tensorflow_backend as kk\n",
    "\n",
    "    model.compile(loss=nnet_survival.surv_likelihood(n_intervals), optimizer=optimizers.RMSprop())\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=20)\n",
    "    history=model.fit(featurespace_train, y_train, batch_size=4063, epochs=100000\n",
    "                      , callbacks=[early_stopping]\n",
    "                      , verbose=0)\n",
    "\n",
    "\n",
    "    # print loss of train and valid data:\n",
    "    # print(model.evaluate(featurespace_train,y_train,verbose=0))\n",
    "    # model.evaluate(featurespace_test,y_test,verbose=0)\n",
    "\n",
    "    #Discrimination performance\n",
    "    # y_pred is conditional prob of survival within each time interval\n",
    "    y_pred_train = model.predict(featurespace_train,verbose=0)\n",
    "    # cumprod = cumulative product, the probability of surviving from time 0 up to the time interval of interest\n",
    "    # index -1 because we are interested in -1 \n",
    "    last_yr_surv_train=np.cumprod(y_pred_train[:,0:np.nonzero(breaks>365*(endpt-1))[0][0]], axis=1)[:,-1]\n",
    "    print('Train C-index fold', str(fold+1),':')\n",
    "    print(concordance_index(train_df.time, last_yr_surv_train, train_df.event)) \n",
    "\n",
    "    y_pred=model.predict(featurespace_test,verbose=0)\n",
    "    last_yr_surv=np.cumprod(y_pred[:,0:np.nonzero(breaks>365*(endpt-1))[0][0]], axis=1)[:,-1]\n",
    "    print('Test C-index fold', str(fold+1),':')\n",
    "    print(concordance_index(test_df.time,last_yr_surv, test_df.event))\n",
    "\n",
    "    pred_surv = np.zeros((len(test_df.event), len(eval_times)))\n",
    "    col=0\n",
    "    for time in eval_times:\n",
    "        pred_surv[:,col] = nnet_pred_surv(y_pred, breaks, time)\n",
    "        col = col+1\n",
    "    pred_surv = pd.DataFrame(data = pred_surv)\n",
    "    # savedir = os.path.join(os.getcwd(),'python_files/csv_files/nnet_survival/all_features')\n",
    "    savedir = os.path.join(work_dir,'csv_files/nnet_survival/'+str(n_features)+'_features/'+term_pred)\n",
    "    try: \n",
    "        os.makedirs(savedir)\n",
    "    except OSError:\n",
    "        if not os.path.isdir(savedir):\n",
    "            raise\n",
    "    actual_fold = fold+1\n",
    "    pred_surv.to_csv(savedir+'/pred_prob_surv_fold_'+str(actual_fold)+'.csv', index = None, header = True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
